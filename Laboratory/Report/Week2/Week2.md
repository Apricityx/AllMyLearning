# Reinforcement Learning

## 马尔科夫奖励过程

四元组<S,P,R,γ >

### 状态集 S

表示状态的集合

### 马尔科夫过程

指的是状态集合之间的转变是逐个推理的过程，未来与过去无关只由现在有关

例如A -> B -> C 则不存在A->C

### 状态转移概率矩阵 P

从某状态转移到某状态的概率组成的矩阵

### 奖励函数 R

从一个状态转移为下一个状态的奖励

$R_s=E [R_{t+1} | S_t = s]$

$R_t$和$R_s$不同，$R_t$人工定义，而$R_s$是由$R_t$定义的

其中E代表求后面式子的均值（期望）

即时刻为t，状态为s时所有转移概率带来奖励的均值为$R_s$

### 回报与衰减系数 γ

时间距离现在越长，奖励倍率越低

值函数

$V(s) = E[G_t | S_t = s]$

$G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$

$V(s)$为$G_t$取均值

### 马尔科夫决策过程

马尔科夫决策过程是一个五元组

$S$ 状态集

$A$ 动作集

$P$ 状态转移概率矩阵

$R$ 奖励函数

$γ$ 折扣因子 衰减系数

$$
P^{\pi}_{s,s'} = \sum_{a \in A} \pi(a | s) P^a_{s,s'}
$$

$$
R^{\pi}_{s} = \sum_{a \in A} \pi(a | s) R^a_s
$$

$P^a_{s,s'}$是指从状态s到状态s'转移过程中，转移策略为$a$的情况下的状态转移矩阵，可理解为s采取a行为的概率乘以采取a行为能达到s‘的概率

$\pi(a|s)$可以理解为s采取a行为的概率

#### 状态值函数

$$
V(s) = E_\pi[G_t | S_t = s]
$$

衡量的是在状态 $s$ 下，按照策略 $\pi$ 行动，未来能获得的累计奖励的期望值，是某一个状态的价值

#### 动作值函数

$$
q_\pi(s,a) = E[G_t | S_t = s,A_t = a]
$$

在策略$\pi$下，从状态$s$选择动作$a$后，未来能获得的期望累计奖励，是某一个动作的价值

#### 值函数

$$
v_\pi(s) = \sum_{a \in A}\pi(a|s)q_\pi(s,a)
$$

值函数表示了某个状态下所有动作的价值
